---
title: "AAI1001 Team 7 Project Proposal"
author:
  - name: Guo Zi Qiang Robin
  - name: Chew Tze Han
  - name: Cheong Wai Hong Jared
  - name: Akram
  - name: Gregory Tan

format:
  html:
    embed-resources: true
    theme: lux
    toc: true 
---

# Project Proposal
Please ensure that you have installed these packages in your R environment before running the code chunks below.

```{r, message=FALSE, warning=FALSE, echo=TRUE}
# Load necessary libraries
library(tidyverse) # for data manipulation and visualization
library(metR) # for contour labels
library(viridis) # for color scales
library(ggpp) # for position_dodgenudge 
library(ggrepel) # for text repelling
library(directlabels) # for categorical text labeling
library(ggforce) # for enclosing shapes
library(VIM) # for NA value prediction
```

# Introduction
Using data engineering and visualisation with various packages in R, we will create a poster that accurately and thoughtfully displays the socioeconomic factors that influence fertility/birth rates in Singapore. To do so we will be using data sourced from [data.gov.sg](https://data.gov.sg/datasets/d_733c6493145ea436f49e9af17a922a58/view){target="blank"} as well as [kaggle](https://www.kaggle.com/datasets/harveytan/singapore-historical-fertility-rate){target="blank"}.

## Analysis of Original Visualisations
The original visualisations from the Straits Times are shown below,

![Total fertility rate from 2019 to 2023](imgs/total_fertility_rate.png)

![Number of births from 2019 to 2023](imgs/num_births.png)
Visualisations adapted from *[Straits Times: Singapore’s total fertility rate hits record low in 2023, falls below 1 for first time](https://www.straitstimes.com/singapore/politics/singapore-s-total-fertility-rate-hits-record-low-in-2023-falls-below-1-for-first-time){target="_blank"}*

The original visualisations focus on the total fertility rate and the number of births in Singapore from 2019 to 2023. While these visualisations provide a clear overview of the declining fertility trend, they lack depth in exploring the underlying socioeconomic factors that contribute to this trend.

In a more recent article, *[Straits Times: Why the fertility rate doesn’t capture socio-economic or cultural trends](https://www.straitstimes.com/singapore/why-the-fertility-rate-doesn-t-capture-socio-economic-or-cultural-trends){target="_blank"}*, it critiques the visualizations represented, stating that the first article statistically misrepresented the data to show that single women are having less kids but not accounting for other socioeconomic factors that led to the decline in fertility rate.

## Proposed Improvements

These are the improvements that our group have chosen to propose to improve on the first article by including the following dimensions:

1. **Monthly work hours**: To show the relationship between work hours and fertility rates.

2. **Monthly Income**: To show the relationship between income and fertility rates, as well as the impact of economic factors on family planning decisions.

3. **Gender**

4. **Fertility Rate by Year & Age**: To show the fertility rate by age group and year, highlighting trends in fertility across different age demographics.

Overall, we aim to present a more balanced viewpoint that shows the economical and employment burdens of Singaporeans by gender.


# Data Engineering
Using the 'tidyverse' package, we will perform data engineering to clean and prepare the data for visualisation. This includes:

- converting column names to be consistent (eg. lower case and consistent naming for age banding)

- collapse age bands like "15 - 19" and "20 - 24" into Below 30 to match employment_details

- filtering out any anomalies or outliers in the data

- Pivoting from wide to long format for Visualisations

- merging datasets

- ensuring value is numeric

- split variables like "NumberOfHoursWorked_30_34_Females" into separate columns

- standard unit of measurement (uom)

## Data Loading

```{r}
#| echo: true
#| eval: true
employment_details <- read_csv("datasets/ResidentWorkingPersonsAged15YearsandOverbyMonthlyIncomefromWorkNumberofHoursWorkedandSexCensusofPopulation2000.csv")

fertility_by_age <- read_csv("datasets/total_fertility_rate_by_age.csv")
fertility_by_race <- read_csv("datasets/total_fertility_rate_by_race.csv")
```

## Cleaning
There are "na" in the original fertility_by_age dataset these are coerced to NA. IDK what to do with them or if is correct to remove for now i leave

### employment_details
```{r}
#| echo: true
#| eval: true

# drop totals in emp2
emp2 <- employment_details |> 
  select(-Total_Total, -Total_Males, -Total_Females)

# pivot emp2 longer and rename age_group to be consistent with fertility datasets
employment_long <- emp2 |>
  pivot_longer(
    cols       = starts_with("NumberOfHoursWorked"),
    names_to   = c("age_group", "gender"),
    names_pattern = "NumberOfHoursWorked_([^_]+(?:_[^_]+)?)_(.*)",
    values_to  = "count"
  ) |>
  # normalize labels
  mutate(
    gender    = str_to_title(gender),           # “males”→“Males”
    age_group = str_replace_all(age_group, 
                  c("Below30" = "Below 30",
                    "65andOver" = "65+",
                    "_"       = "-"))
  ) |>
  # rename Number for clarity
  rename(Monthly_Income = Number)
```

### fertility_by_age
The original dataset has inconsistent age banding and units of measurement. We will clean the age bands, normalize hyphens, and map them to a unified age group. Additionally, we will rename the `value` column to `births` for clarity.
```{r}
#| echo: true
#| eval: true


fertility_age_clean <- fertility_by_age |>
  # 1. Drop the " Years" suffix and collapse extra whitespace
  mutate(
    age_band = str_remove(age_band, " Years"),
    age_band = str_squish(age_band),
    # 2. Normalize all hyphens: "15 - 19", "25  - 29" → "15-19", "25-29"
    age_band = str_replace_all(age_band, "\\s*-\\s*", "-"),
    births = as.numeric(value) # rename value to births
  ) |>
  # 3. Map every age_band into your unified age_group
  mutate(
    age_group = case_when(
      age_band %in% c("15-19", "20-24", "25-29") ~ "Below 30",
      age_band == "30-34"                         ~ "30-34",
      age_band == "35-39"                         ~ "35-39",
      age_band == "40-44"                         ~ "40-44",
      age_band == "45-49"                         ~ "45-49",
      TRUE                                        ~ NA_character_
    )
  ) |>
  # 4. Drop any rows that didn’t match
  filter(!is.na(age_group)) |>
  # 5. Ensure numeric values (NAs retained)
  mutate(
    value = as.numeric(value)
  ) |>
  select(-age_band)
  # 6. drop age_band
```

#### Outliers
Checking for outliers within fertility_by_age dataset using the IQR method. This'll help us identify any extreme values that might skew our analysis. Outliers may be either removed or handled through imputation methods later on.
```{r}

# Checking for outliers (using IQR)

# IQR calculation
Q1 <- quantile(fertility_age_clean$value, 0.25, na.rm = TRUE)
Q3 <- quantile(fertility_age_clean$value, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR

# Identify outliers
outliers <- fertility_age_clean %>%
  filter(value < lower_bound | value > upper_bound) %>%
  arrange(value)

# Visualise outliers
ggplot(fertility_age_clean, aes(y = value)) +
  geom_boxplot(outlier.color = "red", na.rm = TRUE) +
  labs(title = "Fertility Rate Outliers (IQR Method)", y = "Fertility Rate") +
  theme_minimal()

print(nrow(outliers))
```
#### NA values
There are a few methods in which we can deal with NA values:
- complete removal of rows with NA values
- mean/median imputation
- predictive imputation (KNN)

```{r}

# Removal of NA values

fertility_by_age_dropped <- na.omit(fertility_age_clean)  # Removes all rows with NAs

colSums(is.na(fertility_by_age_dropped))

# Median imputation

fertility_age_medianimputed <- fertility_age_clean |>
  mutate(births = ifelse(is.na(births), median(births, na.rm = TRUE), births))

colSums(is.na(fertility_age_medianimputed))

# Predictive imputation (using KNN)

fertility_by_age_KNNimputed <- kNN(fertility_age_clean, variable = "births")

colSums(is.na(fertility_by_age_KNNimputed))
```

```{r}

# Compare the three methods to handle NA values

mean_dropped <- mean(fertility_by_age_dropped$births, na.rm = TRUE)
median_dropped <- median(fertility_by_age_dropped$births, na.rm = TRUE)

mean_medianimputed <- mean(fertility_age_medianimputed$births, na.rm = TRUE)
median_medianimputed <- median(fertility_age_medianimputed$births, na.rm = TRUE)

mean_KNNimputed <- mean(fertility_by_age_KNNimputed$births, na.rm = TRUE)
median_KNNimputed <- median(fertility_by_age_KNNimputed$births, na.rm = TRUE)


comparison <- data.frame(
  Method = c("Dropped NAs", "Median Imputation", "KNN Imputation"),
  Mean_Births = c(mean_dropped, mean_medianimputed, mean_KNNimputed),
  Median_Births = c(median_dropped, median_medianimputed, median_KNNimputed)
)

print(comparison)


```
Each method has its own advantages and disadvantages:

- **Complete Removal**: This method is straightforward but can lead to loss of valuable data, of the 420 rows, 20 have na values, a 5% loss of data. Additionally, with how the dataset is structured not being entirely raw data, each row represents a specific age group and year, so removing rows could lead to gaps in the data.

- **Median Imputation**: This method is robust against outliers and preserves the overall distribution of the data. Maintaining all rows keeps the dataset intact, allowing for a more comprehensive analysis. However, it may not capture the underlying patterns as accurately as other methods.

- **KNN Imputation**: This method uses the nearest neighbors to predict missing values, which produces more accurate results than median imputation. Though it is mostly used in machine learning, its use here is substantial in maintaining all rows for a complete dataset with reasonable predictions of missing values. Though, it requires more computational resources and may introduce noise if the data is not well-structured. The former is non consequential as our dataset is small enough, however the same small data means that there is less 'neighbouring' data to predict from, which may lead to less accurate predictions.

All methods have their own benefits and drawbacks. To not tunnel vision ourselves, it may prove beneficial to perform analysis of data with all 3 methods and compare the results to see if they yield similar insights or if one method stands out as more effective for our specific analysis.

### fertility_by_race
Original dataset used a different uom than fertility_clean, so we will change the unit of measurement to match fertility_clean. Aditionally, rename `value` to `births` for clarity.

```{r}
#| echo: true
#| eval: true

# Changing unit of measurement in fertility_by_race from "Per Female" to "Per Thousand Females" to match fertility_clean
### fertility_by_race

fertility_race_clean <- fertility_by_race |>
  # 1. Convert to numeric & scale up to per 1,000 females
  mutate(
    births = as.numeric(value) * 1000,
    uom                = "Per Thousand Females"
  ) |>
  # 2. Keep only the cleaned columns
  select(year, race, births, uom)
```




